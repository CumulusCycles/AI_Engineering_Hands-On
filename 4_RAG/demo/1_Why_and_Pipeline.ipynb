{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Why RAG? + Minimal Pipeline\n",
    "\n",
    "**Why RAG:** The same question answered without context vs with context—without context the LLM may guess or hallucinate; with context the answer stays grounded in your data.\n",
    "\n",
    "**Pipeline:** Load document → chunk → embed → store in ChromaDB → retrieve top-k → prompt LLM → answer.\n",
    "\n",
    "**Next:** Notebook 2 runs several concrete RAG examples using the same utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "TOP_K = 5  # we'll compare k=5 vs k=8 in the pipeline below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why RAG? Same question, two ways\n",
    "\n",
    "We use a short excerpt as \"retrieved context\" and ask one question: first **without** context, then **with** context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"\n",
    "AI Agent Insure offers a portfolio of AI-native insurance products structured around distinct AI risk domains:\n",
    "- AI Infrastructure & Operations Protection\n",
    "- Agentic AI Liability Insurance\n",
    "- Autonomous Systems & Robotics Coverage\n",
    "- Compliance & Regulatory Shield\n",
    "- Agentic Workflow Uptime Insurance\n",
    "- Intellectual Property & Output Protection\n",
    "- Model & Data Security Insurance\n",
    "- AI Incident Response & Crisis Management\n",
    "- Synthetic Data & Dataset Integrity Coverage\n",
    "\"\"\".strip()\n",
    "\n",
    "question = \"What products does AI Agent Insure offer?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without context (raw LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (no context):\n",
      "As of my last update in October 2023, AI Agent Insure is not a widely recognized company or brand in the insurance industry. It's possible that it could be a new or niche provider that has emerged since then, or it may refer to a specific product or service related to AI in the insurance sector.\n",
      "\n",
      "If AI Agent Insure is a company that has launched recently, I recommend checking their official website or contacting them directly for the most accurate and up-to-date information regarding their products and services. Typically, insurance companies may offer products such as:\n",
      "\n",
      "1. **Auto Insurance**\n",
      "2. **Homeowners Insurance**\n",
      "3. **Renters Insurance**\n",
      "4. **Life Insurance**\n",
      "5. **Health Insurance**\n",
      "6. **Business Insurance**\n",
      "7. **Travel Insurance**\n",
      "8. **Disability Insurance**\n",
      "\n",
      "If you have more specific information or context about AI Agent Insure, I would be happy to help further!\n"
     ]
    }
   ],
   "source": [
    "response_no = client.chat.completions.create(\n",
    "    model=CHAT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    temperature=0\n",
    ")\n",
    "print(\"Answer (no context):\")\n",
    "print(response_no.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With context (RAG-style)\n",
    "\n",
    "Same **prompt template** for both demos; only the question changes. First: a question the context answers. Second: a question it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What products does AI Agent Insure offer?\n",
      "Answer (with context): AI Agent Insure offers the following products:\n",
      "- AI Infrastructure & Operations Protection\n",
      "- Agentic AI Liability Insurance\n",
      "- Autonomous Systems & Robotics Coverage\n",
      "- Compliance & Regulatory Shield\n",
      "- Agentic Workflow Uptime Insurance\n",
      "- Intellectual Property & Output Protection\n",
      "- Model & Data Security Insurance\n",
      "- AI Incident Response & Crisis Management\n",
      "- Synthetic Data & Dataset Integrity Coverage\n"
     ]
    }
   ],
   "source": [
    "def prompt_with_context(context: str, q: str) -> str:\n",
    "    return f\"\"\"Use only the following context to answer the question. If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Demo 1: question the context answers\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=CHAT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_with_context(CONTEXT, question)}],\n",
    "    temperature=0\n",
    ")\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer (with context):\", response_1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who won the 2025 F1 Constructor's Championship?\n",
      "Answer (with context): The context does not contain the answer to the question about the 2025 F1 Constructor's Championship.\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: question the context does *not* answer (same template, different question)\n",
    "new_question = \"Who won the 2025 F1 Constructor's Championship?\"\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=CHAT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_with_context(CONTEXT, new_question)}],\n",
    "    temperature=0\n",
    ")\n",
    "print(\"Question:\", new_question)\n",
    "print(\"Answer (with context):\", response_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without context the model may guess or hallucinate. With context the answer is grounded in the document. **RAG** is the system that retrieves that context automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal pipeline: load → chunk → embed → store → retrieve → generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same question, two retrieval settings: **top-5** vs **top-8**. With 5 chunks the one containing the product list often isn't in the retrieved set; with 8 it usually is. So the answer can go from \"context doesn't contain the answer\" to the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4994 chars, 34 chunks\n"
     ]
    }
   ],
   "source": [
    "from rag_utils import load_document, chunk_by_sentences, embed, build_index\n",
    "\n",
    "doc = load_document()\n",
    "chunks = chunk_by_sentences(doc)\n",
    "print(f\"Loaded {len(doc)} chars, {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built.\n"
     ]
    }
   ],
   "source": [
    "chroma_client, coll = build_index(chunks, collection_name=\"rag\", client=client)\n",
    "print(\"Index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_query(coll, question: str, k: int, client):\n",
    "    \"\"\"Retrieve top-k chunks, build prompt with context, return (answer, retrieved_chunks).\"\"\"\n",
    "    q_emb = embed([question], client=client)[0]\n",
    "    results = coll.query(query_embeddings=[q_emb], n_results=k)\n",
    "    retrieved = results[\"documents\"][0]\n",
    "    context = \"\\n\".join(retrieved)\n",
    "    prompt = prompt_with_context(context, question)\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content, retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-5 retrieval ---\n",
      "Answer: The context does not specify the exact products offered by AI Agent Insure. It mentions tailored insurance solutions addressing various risks related to AI systems, but does not list specific products.\n"
     ]
    }
   ],
   "source": [
    "# Top-5: chunk with product list often not in the retrieved set\n",
    "answer_5, retrieved_5 = run_rag_query(coll, question, 5, client)\n",
    "print(\"--- Top-5 retrieval ---\")\n",
    "print(\"Answer:\", answer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-8 retrieval ---\n",
      "Answer: AI Agent Insure offers a portfolio of AI-native insurance products structured around distinct AI risk domains, including:\n",
      "\n",
      "- AI Infrastructure & Operations Protection\n",
      "- Agentic AI Liability Insurance\n",
      "- Autonomous Systems & Robotics Coverage\n",
      "- Compliance & Regulatory Shield\n",
      "- Agentic Workflow Uptime Insurance\n",
      "- Intellectual Property & Output Protection\n",
      "- Model & Data Security Insurance\n",
      "- AI Incident Response & Crisis Management\n",
      "- Synthetic Data & Dataset Integrity Coverage\n"
     ]
    }
   ],
   "source": [
    "# Top-8: product-list chunk usually included → full answer\n",
    "answer_8, retrieved_8 = run_rag_query(coll, question, 8, client)\n",
    "print(\"--- Top-8 retrieval ---\")\n",
    "print(\"Answer:\", answer_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Retrieval quality (and **k**) directly affects whether the LLM can answer. Too small a k and the relevant chunk may be missed; larger k improves recall but adds noise and cost. Tune k (and chunking) for your data and queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
