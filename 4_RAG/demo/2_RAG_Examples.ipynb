{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: RAG Examples\n",
    "\n",
    "Concrete RAG examples using the shared pipeline (see Notebook 1). We reuse `rag_utils` so there's no duplicate setup.\n",
    "\n",
    "- **Example 1:** Single-doc Q&A (product list)\n",
    "- **Example 2:** Answer with in-text citations (which chunk)\n",
    "- **Example 3:** Different question types (underwriting, target market)\n",
    "- **Chunking:** Sentence vs token-based in one place\n",
    "- **Evaluation:** When to measure retrieval (Precision@K, hit rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 chunks, index ready.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from rag_utils import load_document, chunk_by_sentences, embed, build_index\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "TOP_K = 8\n",
    "\n",
    "doc = load_document()\n",
    "chunks = chunk_by_sentences(doc)\n",
    "chroma_client, coll = build_index(chunks, collection_name=\"rag\", client=client)\n",
    "print(f\"Loaded {len(chunks)} chunks, index ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single-doc Q&A — product list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What products does AI Agent Insure offer?\n",
      "A: AI Agent Insure offers a portfolio of AI-native insurance products structured around distinct AI risk domains, including:\n",
      "\n",
      "- AI Infrastructure & Operations Protection\n",
      "- Agentic AI Liability Insurance\n",
      "- Autonomous Systems & Robotics Coverage\n",
      "- Compliance & Regulatory Shield\n",
      "- Agentic Workflow Uptime Insurance\n",
      "- Intellectual Property & Output Protection\n",
      "- Model & Data Security Insurance\n",
      "- AI Incident Response & Crisis Management\n",
      "- Synthetic Data & Dataset Integrity Coverage\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What products does AI Agent Insure offer?\"\n",
    "q_emb = embed([question1], client=client)[0]\n",
    "res = coll.query(query_embeddings=[q_emb], n_results=TOP_K)\n",
    "retrieved = res[\"documents\"][0]\n",
    "ctx = \"\\n\".join(retrieved)\n",
    "\n",
    "prompt = f\"\"\"Use only the following context to answer. If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {question1}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "ans = client.chat.completions.create(model=CHAT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0)\n",
    "print(\"Q:\", question1)\n",
    "print(\"A:\", ans.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Answer with in-text citations\n",
    "\n",
    "We number the retrieved chunks and ask the model to cite which chunk (e.g. Chunk 11) it used for each fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is AI Agent Insure's underwriting philosophy?\n",
      "A: AI Agent Insure's underwriting philosophy evaluates risk at the intersection of technical architecture, operational controls, and governance (Chunk 12).\n"
     ]
    }
   ],
   "source": [
    "question2 = \"What is AI Agent Insure's underwriting philosophy?\"\n",
    "q_emb = embed([question2], client=client)[0]\n",
    "res = coll.query(query_embeddings=[q_emb], n_results=TOP_K)\n",
    "retrieved_docs = res[\"documents\"][0]\n",
    "retrieved_ids = res[\"ids\"][0]\n",
    "\n",
    "numbered_ctx = \"\\n\".join([f\"[Chunk {i}] {d}\" for i, d in zip(retrieved_ids, retrieved_docs)])\n",
    "prompt_cite = f\"\"\"Use only the following context. For each fact you state, cite the source in parentheses, e.g. (Chunk 0). If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{numbered_ctx}\n",
    "\n",
    "Question: {question2}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "ans = client.chat.completions.create(model=CHAT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt_cite}], temperature=0)\n",
    "print(\"Q:\", question2)\n",
    "print(\"A:\", ans.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Different question — target market\n",
    "\n",
    "Same index, different query. Shows retrieval finding the right section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who are AI Agent Insure's target market segments?\n",
      "A: AI Agent Insure's target market segments include:\n",
      "\n",
      "- AI startups and LLM application providers\n",
      "- Enterprises deploying autonomous agents and workflows\n",
      "- Robotics and autonomous vehicle developers\n",
      "- Synthetic data and model training organizations\n",
      "- Regulated industries adopting AI (healthcare, finance, legal)\n"
     ]
    }
   ],
   "source": [
    "question3 = \"Who are AI Agent Insure's target market segments?\"\n",
    "q_emb = embed([question3], client=client)[0]\n",
    "res = coll.query(query_embeddings=[q_emb], n_results=TOP_K)\n",
    "retrieved = res[\"documents\"][0]\n",
    "ctx = \"\\n\".join(retrieved)\n",
    "\n",
    "prompt = f\"\"\"Use only the following context to answer. If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {question3}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "ans = client.chat.completions.create(model=CHAT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0)\n",
    "print(\"Q:\", question3)\n",
    "print(\"A:\", ans.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking: sentence vs token-based\n",
    "\n",
    "Different chunking changes which chunks exist and what gets retrieved. Below we **compare** both strategies: same question, two indexes (sentence vs token), then retrieval + answer for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence chunks: 34\n",
      "Token chunks (80 tok, 20 overlap): 15\n",
      "Sample token chunk: AI Liability Insurance\n",
      "- Autonomous Systems & Robotics Coverage\n",
      "- Compliance & Regulatory Shield\n",
      "- A...\n"
     ]
    }
   ],
   "source": [
    "from rag_utils import chunk_by_tokens\n",
    "\n",
    "chunks_sent = chunk_by_sentences(doc)\n",
    "chunks_tok = chunk_by_tokens(doc, max_tokens=80, overlap_tokens=20)\n",
    "print(f\"Sentence chunks: {len(chunks_sent)}\")\n",
    "print(f\"Token chunks (80 tok, 20 overlap): {len(chunks_tok)}\")\n",
    "print(\"Sample token chunk:\", chunks_tok[6][:100] + \"...\" if len(chunks_tok[6]) > 100 else chunks_tok[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTENCE CHUNKING\n",
      "============================================================\n",
      "Retrieved (preview): ['Company Overview  AI Agent Insure is a specialty i...', 'AI Agent Insure provides tailored insurance soluti...', '# AI Agent Insure ## Comprehensive Company Profile...']\n",
      "Answer: AI Agent Insure offers a portfolio of AI-native insurance products structured around distinct AI risk domains, including:\n",
      "\n",
      "- AI Infrastructure & Operations Protection\n",
      "- Agentic AI Liability Insurance\n",
      "- Autonomous Systems & Robotics Coverage\n",
      "- Compliance & Regulatory Shield\n",
      "- Agentic Workflow Uptime Insurance\n",
      "- Intellectual Property & Output Protection\n",
      "- Model & Data Security Insurance\n",
      "- AI Incident Response & Crisis Management\n",
      "- Synthetic Data & Dataset Integrity Coverage\n",
      "\n",
      "============================================================\n",
      "TOKEN CHUNKING (80 tok, 20 overlap)\n",
      "============================================================\n",
      "Retrieved (preview): ['The company is designed as an AI-native insurer ra...', 'AI Liability Insurance\\n- Autonomous Systems & Robo...', 'AI retrieval systems, including vector databases a...']\n",
      "Answer: AI Agent Insure offers the following products:\n",
      "\n",
      "- Autonomous Systems & Robotics Coverage\n",
      "- Compliance & Regulatory Shield\n",
      "- Agentic Workflow Uptime Insurance\n",
      "- Intellectual Property & Output Protection\n",
      "- Model & Data Security Insurance\n",
      "- AI Incident Response & Crisis Management\n",
      "- Synthetic Data & Dataset Integrity Coverage\n"
     ]
    }
   ],
   "source": [
    "# Build separate indexes; same question for both strategies\n",
    "_, coll_sent = build_index(chunks_sent, collection_name=\"rag_sentence\", client=client)\n",
    "_, coll_tok = build_index(chunks_tok, collection_name=\"rag_token\", client=client)\n",
    "compare_q = \"What products does AI Agent Insure offer?\"\n",
    "q_emb = embed([compare_q], client=client)[0]\n",
    "\n",
    "res_sent = coll_sent.query(query_embeddings=[q_emb], n_results=TOP_K)\n",
    "res_tok = coll_tok.query(query_embeddings=[q_emb], n_results=TOP_K)\n",
    "retrieved_sent = res_sent[\"documents\"][0]\n",
    "retrieved_tok = res_tok[\"documents\"][0]\n",
    "\n",
    "def answer_from_chunks(chunks, question):\n",
    "    ctx = \"\\n\".join(chunks)\n",
    "    prompt = f\"\"\"Use only the following context to answer. If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    r = client.chat.completions.create(model=CHAT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0)\n",
    "    return r.choices[0].message.content\n",
    "\n",
    "answer_sent = answer_from_chunks(retrieved_sent, compare_q)\n",
    "answer_tok = answer_from_chunks(retrieved_tok, compare_q)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTENCE CHUNKING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Retrieved (preview):\", [d[:50] + \"...\" for d in retrieved_sent[:3]])\n",
    "print(\"Answer:\", answer_sent)\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKEN CHUNKING (80 tok, 20 overlap)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Retrieved (preview):\", [d[:50] + \"...\" for d in retrieved_tok[:3]])\n",
    "print(\"Answer:\", answer_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Sentence chunking yields more, smaller chunks (full sentences); token chunking yields fewer, fixed-size chunks that can cut across sentences. Retrieval and the final answer can differ. Tune chunk size and overlap for your docs and queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
