{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM (Ollama): Ollama Chat & OpenAI Chat Completions\n",
    "\n",
    "This notebook demonstrates two different ways to interact with local LLMs using Ollama:\n",
    "- **Ollama Chat API**: The native Ollama Python library - simple and direct\n",
    "- **OpenAI Chat Completions API Format**: Use OpenAI's client library with Ollama's OpenAI-compatible endpoint - useful for code compatibility\n",
    "\n",
    "Both methods call the same local Ollama models running on your machine!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Create Virtual Environment\n",
    "\n",
    "    Example:\n",
    "```bash\n",
    "        uv venv --python=python3.12\n",
    "        source .venv/bin/activate\n",
    "        uv pip install -r requirements.txt -q\n",
    "```\n",
    "\n",
    "- For Ollama:\n",
    "  - Ensure [Ollama is installed](https://ollama.ai/download)\n",
    "  - Start Ollama by running: `ollama serve` in your Terminal\n",
    "  - Pull a model (e.g., `llama3.2:3b`) by running: `ollama pull llama3.2:3b`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ollama Chat (Local LLM)\n",
    "\n",
    "Ollama allows you to run large language models locally on your machine. This provides:\n",
    "- **Privacy**: Your data never leaves your machine\n",
    "- **No API costs**: Free to use once installed\n",
    "- **Offline capability**: Works without internet connection\n",
    "- **Customization**: Run any model you download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Ollama Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Ollama Chat Request\n",
    "\n",
    "**What:** Simple single-message chat with a local model.\n",
    "\n",
    "**Why:** Demonstrates the basic structure of Ollama's chat API - similar to OpenAI but runs locally.\n",
    "\n",
    "**How:** Use `ollama.chat()` with a model name and messages list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Response:\n",
      "Machine learning (ML) is a subset of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It involves training algorithms on large datasets, allowing them to identify patterns and make predictions or decisions based on the information they've learned. This process enables machines to become increasingly accurate and autonomous over time.\n"
     ]
    }
   ],
   "source": [
    "# Basic chat request with Ollama\n",
    "# Using llama3.2:3b - a small, fast model good for demos\n",
    "# You can use other models like llama3.2, mistral, codellama, etc.\n",
    "\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2:3b',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What is machine learning? Explain in 2-3 sentences.'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Ollama Response:\")\n",
    "    print(response['message']['content'])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running (ollama serve) and the model is pulled (ollama pull llama3.2:3b)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ollama with System Message\n",
    "\n",
    "**What:** Adding a system message to control the assistant's behavior.\n",
    "\n",
    "**Why:** System messages help set the tone, style, and constraints for the model.\n",
    "\n",
    "**How:** Include a message with `role: \"system\"` in the messages list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Response (with system message):\n",
      "Machine learning (ML) is a type of artificial intelligence (AI) that enables computers to learn from data and improve their performance on a task without being explicitly programmed.\n",
      "\n",
      "Think of it like this: Imagine you're trying to teach a child to recognize different types of animals. You show them pictures of dogs, cats, birds, etc., and they try to classify each one as \"dog\" or \"cat\". At first, they might get some of the answers wrong, but with more practice and exposure to more data (pictures), they become better at recognizing the differences.\n",
      "\n",
      "Machine learning works in a similar way. You provide computers with large amounts of data, such as images, text, or audio files, and let them learn from it to make predictions, classify objects, or identify patterns.\n",
      "\n",
      "There are three main types of machine learning:\n",
      "\n",
      "1. **Supervised Learning**: The computer is shown labeled data (e.g., pictures of dogs with labels \"dog\") and learns to recognize the patterns.\n",
      "2. **Unsupervised Learning**: The computer explores a large dataset without any pre-defined labels, trying to find hidden patterns or relationships.\n",
      "3. **Reinforcement Learning**: The computer learns by interacting with an environment and receiving feedback (rewards or penalties) based on its actions.\n",
      "\n",
      "Machine learning has many applications in areas like:\n",
      "\n",
      "* Image recognition\n",
      "* Natural Language Processing (NLP)\n",
      "* Speech recognition\n",
      "* Predictive maintenance\n",
      "* Personalized recommendations\n",
      "\n",
      "Overall, machine learning is a powerful tool that enables computers to learn from data and improve their performance over time, making it an essential part of modern technology.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful assistant that explains technical concepts in simple terms.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is machine learning?'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Ollama Response (with system message):\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Turn Conversation with Ollama\n",
    "\n",
    "**What:** Building a conversation by maintaining message history.\n",
    "\n",
    "**Why:** Demonstrates context retention - the model remembers previous exchanges.\n",
    "\n",
    "**How:** Include all previous messages (user and assistant) in the messages list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Response:\n",
      "**Calculating Factorial with a Simple Loop**\n",
      "\n",
      "Here is an example of a simple Python function that calculates the factorial of a given number using a loop:\n",
      "\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial for.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of the given number.\n",
      "    \"\"\"\n",
      "    result = 1\n",
      "    for i in range(1, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "number = 5\n",
      "factorial_result = calculate_factorial(number)\n",
      "print(f\"The factorial of {number} is: {factorial_result}\")\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "*   We define a function `calculate_factorial` that takes an integer `n` as input.\n",
      "*   We initialize a variable `result` to 1, which will store the factorial result.\n",
      "*   We use a for loop to iterate from 1 to `n` (inclusive).\n",
      "*   Inside the loop, we multiply the current value of `result` by the current iteration number `i`.\n",
      "*   After the loop finishes, we return the final value of `result`, which is the factorial of the input number.\n",
      "*   We demonstrate how to use this function with an example usage.\n",
      "\n",
      "This implementation uses a simple iterative approach to calculate the factorial, making it easy to understand and efficient for large inputs.\n"
     ]
    }
   ],
   "source": [
    "# First turn\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a helpful coding assistant.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write a simnple, non-recursive Python function to calculate factorial.'\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "assistant_message = response['message']['content']\n",
    "print(\"First Response:\")\n",
    "print(assistant_message)\n",
    "\n",
    "# Add assistant's response to the conversation\n",
    "messages.append({\n",
    "    'role': 'assistant',\n",
    "    'content': assistant_message\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Response (with context):\n",
      "**Calculating Factorial using Recursion**\n",
      "\n",
      "Here is an example of a Python function that calculates the factorial of a given number using recursion:\n",
      "\n",
      "```python\n",
      "def calculate_factorial_recursive(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number recursively.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial for.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of the given number.\n",
      "    \"\"\"\n",
      "    if n == 0 or n == 1:\n",
      "        # Base case: factorial of 0 and 1 is 1\n",
      "        return 1\n",
      "    else:\n",
      "        # Recursive case: factorial of n is n * factorial of (n-1)\n",
      "        return n * calculate_factorial_recursive(n - 1)\n",
      "\n",
      "# Example usage:\n",
      "number = 5\n",
      "factorial_result = calculate_factorial_recursive(number)\n",
      "print(f\"The factorial of {number} is: {factorial_result}\")\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "*   We define a function `calculate_factorial_recursive` that takes an integer `n` as input.\n",
      "*   We check if the input number `n` is 0 or 1. If so, we return 1 (since the factorial of 0 and 1 is defined to be 1).\n",
      "*   For all other values of `n`, we recursively call `calculate_factorial_recursive` with `n - 1` as input.\n",
      "*   We multiply the current value of `n` by the result of the recursive call to compute the factorial of `n`.\n",
      "*   After each recursive call, we return the result, which is then propagated up the call stack until we reach the base case.\n",
      "\n",
      "Note that while recursion can be an elegant way to solve problems like this, it can also lead to stack overflows for very large inputs. In Python, this function should not overflow due to the maximum recursion depth limit being quite high. However, in languages with lower limits (like Java), you would need to implement a loop instead to avoid such issues.\n",
      "\n",
      "**Iterative vs Recursive**\n",
      "\n",
      "While both approaches are valid, there's an important difference between them:\n",
      "\n",
      "*   **Efficiency:** Recursive solutions often have higher overhead due to the repeated function calls and returns.\n",
      "*   **Memory usage:** Recursive solutions typically require more memory because of the recursive call stack.\n",
      "*   **Readability:** Recursive solutions can be easier to understand for problems that naturally lend themselves to recursion, but harder to grasp for those who are not familiar with it.\n",
      "\n",
      "In general, you should choose between iterative and recursive approaches based on the specific problem's requirements, your personal preference, or any performance constraints.\n"
     ]
    }
   ],
   "source": [
    "# Second turn - the model remembers the previous conversation\n",
    "messages.append({\n",
    "    'role': 'user',\n",
    "    'content': 'Can you make it recursive?'\n",
    "})\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"Second Response (with context):\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ollama Parameters: Temperature\n",
    "\n",
    "**What:** Temperature controls randomness/creativity in responses.\n",
    "\n",
    "**Why:** Lower temperature (0.0-0.3) = more deterministic. Higher temperature (0.7-1.0) = more creative.\n",
    "\n",
    "**How:** Set the `options` parameter with `temperature` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.0 (deterministic):\n",
      "Lines of code descend\n",
      "Logic's gentle, guiding hand\n",
      "Beauty in the byte\n"
     ]
    }
   ],
   "source": [
    "content = \"Write a haiku about coding.\"\n",
    "\n",
    "# Low temperature - deterministic\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': content\n",
    "        }\n",
    "    ],\n",
    "    options={\n",
    "        'temperature': 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Temperature 0.0 (deterministic):\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 1.0 (creative):\n",
      "Pixels dance on screen\n",
      "Code whispers secrets in wind\n",
      "Algorithms sing\n"
     ]
    }
   ],
   "source": [
    "# High temperature - creative\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': content\n",
    "        }\n",
    "    ],\n",
    "    options={\n",
    "        'temperature': 1.0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Temperature 1.0 (creative):\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison: Native Ollama API vs OpenAI-Compatible API\n",
    "\n",
    "### Why Use Each Approach?\n",
    "\n",
    "#### Why Use Native Ollama API?\n",
    "\n",
    "The native Ollama API is the simplest and most direct way to interact with Ollama:\n",
    "\n",
    "- **Simpler syntax**: `ollama.chat()` is more straightforward than `client.chat.completions.create()`\n",
    "- **Direct access**: No need to configure base URLs or worry about API compatibility\n",
    "- **Ollama-specific features**: Access to features that might not be available through the OpenAI-compatible endpoint\n",
    "- **Less overhead**: Fewer layers between your code and Ollama\n",
    "- **Better for learning**: When you're specifically learning Ollama, the native API is more intuitive\n",
    "\n",
    "#### Why Use OpenAI-Compatible API Format?\n",
    "\n",
    "The OpenAI-compatible API format provides several important benefits:\n",
    "\n",
    "- **Code reusability**: If you have existing code written for OpenAI's API, you can use it with Ollama by just changing the `base_url`. This means you can:\n",
    "  - Test your OpenAI code locally with Ollama before deploying\n",
    "  - Switch between OpenAI and Ollama without rewriting code\n",
    "  - Use libraries and frameworks designed for OpenAI's API\n",
    "\n",
    "- **Provider flexibility**: Build your application once and easily switch between:\n",
    "  - Local Ollama models (privacy, no cost)\n",
    "  - OpenAI cloud models (more powerful, requires API key)\n",
    "  - Other OpenAI-compatible providers (Anthropic, etc.)\n",
    "\n",
    "- **Team consistency**: If your team is already familiar with OpenAI's API structure, using the same format with Ollama reduces learning curve\n",
    "\n",
    "- **Library ecosystem**: Many Python libraries and tools are built for OpenAI's API format, and they'll work with Ollama too\n",
    "\n",
    "- **Production readiness**: If you plan to deploy to production and might switch between local and cloud models, using the OpenAI format makes that transition seamless\n",
    "\n",
    "**Example scenario**: You're building an application that needs to work both locally (for development/testing) and in production (using OpenAI). With the OpenAI-compatible format, you can use the same code and just change the `base_url` and `api_key` based on the environment.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Feature | Native Ollama API | OpenAI-Compatible API |\n",
    "|---------|-------------------|----------------------|\n",
    "| **Library** | `ollama` Python package | `openai` Python package |\n",
    "| **Syntax** | `ollama.chat()` | `client.chat.completions.create()` |\n",
    "| **Response Format** | Dictionary with `['message']['content']` | `response.choices[0].message.content` |\n",
    "| **Parameters** | `options={'temperature': 0.7}` | `temperature=0.7` |\n",
    "| **Code Compatibility** | Ollama-specific | Compatible with OpenAI code |\n",
    "| **Base URL** | Built-in (localhost:11434) | Must specify `base_url=\"http://localhost:11434/v1\"` |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **Use Native Ollama API when:**\n",
    "  - You're building Ollama-specific applications\n",
    "  - You prefer simpler, more direct syntax\n",
    "  - You want to use Ollama-specific features\n",
    "  - You're learning or experimenting with Ollama\n",
    "\n",
    "- **Use OpenAI-Compatible API when:**\n",
    "  - You want to reuse existing OpenAI code\n",
    "  - You're building applications that might switch between providers\n",
    "  - You prefer OpenAI's API structure\n",
    "  - You want to maintain compatibility with OpenAI-based codebases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Using OpenAI Chat Completions API Format with Ollama\n",
    "\n",
    "Now that you understand the differences, let's see how to use OpenAI's API format with Ollama. Ollama provides an OpenAI-compatible API endpoint, allowing you to use OpenAI's client library to call local models:\n",
    "- **Same API, local models**: Use familiar OpenAI syntax with your local Ollama models\n",
    "- **Code compatibility**: Easily switch between OpenAI and Ollama by changing the base URL\n",
    "- **No API key needed**: Still runs entirely locally\n",
    "- **Same privacy**: All data stays on your machine\n",
    "\n",
    "### Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to Ollama's OpenAI-compatible endpoint\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_format_available = False\n",
    "\n",
    "# Initialize OpenAI client pointing to Ollama's OpenAI-compatible endpoint\n",
    "# Ollama runs on localhost:11434 by default\n",
    "# The /v1 endpoint provides OpenAI-compatible API\n",
    "try:\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\"  # Ollama doesn't require a real API key, but the client needs one\n",
    "    )\n",
    "    print(\"✓ Connected to Ollama's OpenAI-compatible endpoint\")\n",
    "    openai_format_available = True\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not connect to Ollama: {e}\")\n",
    "    print(\"   Make sure Ollama is running (ollama serve)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Chat Completions (OpenAI Format)\n",
    "\n",
    "**What:** Simple single-message chat using OpenAI's API format, but calling Ollama.\n",
    "\n",
    "**Why:** Demonstrates how to use OpenAI's client library syntax with local Ollama models.\n",
    "\n",
    "**How:** Use `client.chat.completions.create()` with Ollama model name and messages list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response (using OpenAI API format with Ollama):\n",
      "Machine learning (ML) is a subset of artificial intelligence that enables computers to learn and improve on their performance by analyzing data without being explicitly programmed. By recognizing patterns and relationships within the data, ML algorithms can make predictions, classify objects, and even create new insights, all without human intervention. This process is often driven by statistical and mathematical techniques.\n",
      "\n",
      "Tokens used: 108\n"
     ]
    }
   ],
   "source": [
    "if openai_format_available:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2:3b\",  # Using Ollama model name\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is machine learning? Explain in 2-3 sentences.\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Response (using OpenAI API format with Ollama):\")\n",
    "    print(response.choices[0].message.content)\n",
    "    if hasattr(response, 'usage') and response.usage:\n",
    "        print(f\"\\nTokens used: {response.usage.total_tokens}\")\n",
    "else:\n",
    "    print(\"Ollama not available. Skipping this example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chat Completions with System Message (OpenAI Format)\n",
    "\n",
    "**What:** Adding a system message using OpenAI's API format.\n",
    "\n",
    "**Why:** System messages help set the tone, style, and constraints for the model.\n",
    "\n",
    "**How:** Include a message with `role: \"system\"` in the messages list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if openai_format_available:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2:3b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains technical concepts in simple terms.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Response (using OpenAI API format with Ollama, with system message):\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"Ollama not available. Skipping this example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Turn Conversation (OpenAI Format)\n",
    "\n",
    "**What:** Building a conversation using OpenAI's API format.\n",
    "\n",
    "**Why:** Demonstrates context retention - the model remembers previous exchanges.\n",
    "\n",
    "**How:** Include all previous messages (user and assistant) in the messages list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if openai_format_available:\n",
    "    # First turn\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a simnple, non-recursive Python function to calculate factorial.\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2:3b\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    assistant_message = response.choices[0].message.content\n",
    "    print(\"First Response (using OpenAI API format):\")\n",
    "    print(assistant_message)\n",
    "    \n",
    "    # Add assistant's response to the conversation\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "else:\n",
    "    print(\"Ollama not available. Skipping this example.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if openai_format_available:\n",
    "    # Second turn - the model remembers the previous conversation\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Can you make it recursive?\"})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2:3b\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    print(\"Second Response (with context, using OpenAI API format):\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"Ollama not available. Skipping this example.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
